{
  "hash": "455c63637c7d8786b9a82a0f34ed8efa",
  "result": {
    "markdown": "---\ntitle: Bayesian inference for a sequence of coinflips\nauthor: Jakob Hoffmann\ndate: 01/01/2023\nformat: html\n---\n\n## About this tutorial\n\nThis is a tutorial on how to use the `LogDensityProblems.jl` ecosystem for Bayesian inference. Compared to other packages, such as `Turing.jl`, this approach is a bit more low-level, with the upside of being more hackable and being insightful for learning purposes.\n\n\n\n## Setup\n\nFor this exercise, we're interested in performing inference on a simple process where a possibly biased coin is flipped $N=100$ times. More formally, we could state this like so:\n\n$$\ny_i \\sim \\mathrm{Bernoulli}(p) \\textrm{ for } i = 1,2,...,100\n$$\n\nWe start by simulating data from a Bernoulli distribution with the probability of heads set to $p = 0.7$,  which for the inverse problem is going to be the unknown quantity of interest to be inferred from observed data.\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\nusing Distributions\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\nN = 100\np = 0.7\nd = Bernoulli(p)\ndata = rand(d, N);\n```\n:::\n\n\n## Model definition\n\nHaving simulated data for inference, we now proceed to the model definition using the `LogDensityProblems` interface package. We store the flips in a struct called `CoinflipProblem`:\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\nstruct CoinflipProblem\n  flips::Vector{Bool}\nend;\n```\n:::\n\n\n> *Note*: The avid reader will notice that this problem could also be more efficiently represented by making use of independence between the flips, in which case we could just record the total number of flips and the number of heads and make use of the binomial distribution. However, for sake of consistency we here stick to the Bernoulli representation.\n\nThe centerpiece of most modern Bayesian inference methods is the unnormalized log probability density function of the posterior distribution, which indicates how well a given parameter value fits the evidence provided by our data (encoded in the loglikelihood function) and our prior beliefs. For our numerical estimation procedure, it guides our search of the parameter space to obtain a representative set of draws from the posterior distribution.\n\nWe start by specifying the loglikelihood function:\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\nfunction loglik(p::Real, flips::Vector{Bool})\n  sum(y -> logpdf(Bernoulli(p), y), flips)\nend;\n```\n:::\n\n\nFor a given value $p$, the loglikelihood function gives us the sum of the log probability densities of each of our flips under a $\\textrm{Bernoulli}(p)$ distribution.\n\nNext to the likelihood function which makes use of the information from the data, we also need to specify a prior distribution which encodes our prior (before having seen the data) belief about $p$. Here, we're going to be broadly skeptical of extremely biased coins and use a $\\mathrm{Beta}(2,2)$ prior:\n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\nfunction logpri(p::Real)\n  logpdf(Beta(2,2), p)\nend;\n```\n:::\n\n\nHere's a plot of what that looks like:\n\n::: {.cell execution_count=7}\n``` {.julia .cell-code}\nusing CairoMakie\nplot(Beta(2,2))\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n![](coinflip_ldp_files/figure-html/cell-8-output-1.svg){}\n:::\n:::\n\n\nWe now make our problem struct callable on an input parameter $\\theta$, which is just a 'container' holding our actual parameter of interest, $p$, but could also contain other parameters. Calling the struct on a given $\\theta$ returns the sum of the log density of the prior and the log likelihood, a.k.a. the log posterior density evaluated at $p$.\n\n::: {.cell execution_count=8}\n``` {.julia .cell-code}\nfunction (problem::CoinflipProblem)(θ)\n  (; flips) = problem\n  (; p) = θ\n  logpri(p) + loglik(p, flips)\nend  \n```\n:::\n\n\nWe can now instantiate our problem on the data and evaluate the log posterior density at a couple of values for $p$:\n\n::: {.cell execution_count=9}\n``` {.julia .cell-code}\nproblem = CoinflipProblem(data)\nproblem((; p=0.1)), problem((; p=0.5)), problem((; p=0.9))\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n(-160.56350896406923, -68.90925294788643, -81.46342417996544)\n```\n:::\n:::\n\n\nWhile the actual values of the log posterior density are not immediately that useful, we can already infer that, given the data and our prior beliefs, $p=0.5$ is deemed similarly likely compared to $p=0.9$ and much more likely than $p=0.1$.\n\n## Model estimation\nHaving defined a way to evaluate the posterior density for a given parameter value, we now want a full representation of the posterior distribution to draw conclusions about the coin. While for simple problems, like the one presented here, a closed-form analytical solution is available, a more general method which also works for complicated models is to draw a large number of samples from the posterior distribution. Based on these samples, one can easily derive statements about certain summaries of the posterior distribution (e.g., its mean and standard deviation) or visualize it.\n\nA general purpose numerical procedure for obtaining samples from the posterior distribution, using the unnormalized log posterior density function (as specified above) and its gradient, is Hamiltonian Monte Carlo (HMC) and its variants. \n\nAs more of an implementation detail, HMC operates on the unconstrained reals but our parameter $p$ is confined to the unit interval $(0,1)$ so we need an appropriate transformation, which is conveniently available in the `TransformedLogDensities` package. As mentioned, HMC furthermore requires the gradient of the posterior density, which we can conveniently obtain via automatic differentiation, in this case using the `ForwardDiff` package.\n\n::: {.cell execution_count=10}\n``` {.julia .cell-code}\nusing LogDensityProblems\nusing TransformVariables, TransformedLogDensities\nusing LogDensityProblemsAD, ForwardDiff\n\ntransformation = as((p=as_unit_interval,))\ntran = TransformedLogDensity(transformation, problem)\ngrad = ADgradient(:ForwardDiff, tran)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\nForwardDiff AD wrapper for TransformedLogDensity of dimension 1, w/ chunk size 1\n```\n:::\n:::\n\n\nWe can now evaluate the logdensity and its gradient:\n\n::: {.cell execution_count=11}\n``` {.julia .cell-code}\nLogDensityProblems.logdensity_and_gradient(grad, zeros(1))\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n(-70.29554730900632, [18.0])\n```\n:::\n:::\n\n\nWith this in place, we can now draw a large number of samples (say, $S=2000$) from the posterior distribution using the HMC implementation in `DynamicHMC`. We use the `ThreadsX` package to sample $k$ chains in parallel:\n\n::: {.cell execution_count=12}\n``` {.julia .cell-code}\nusing Random\nusing DynamicHMC\nusing ThreadsX\n\nfunction sample(grad, S, k; rng=Random.default_rng()) \n   ThreadsX.map(1:k) do _\n     mcmc_with_warmup(rng, grad, S; reporter=NoProgressReport())\n   end\nend\n\nresult = sample(grad, 2000, 4)\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n4-element Vector{NamedTuple{(:posterior_matrix, :tree_statistics, :κ, :ϵ), Tuple{Matrix{Float64}, Vector{DynamicHMC.TreeStatisticsNUTS}, GaussianKineticEnergy{LinearAlgebra.Diagonal{Float64, Vector{Float64}}, LinearAlgebra.Diagonal{Float64, Vector{Float64}}}, Float64}}}:\n (posterior_matrix = [0.42505989546098893 1.0863757983697473 … 0.814567548310582 0.814567548310582], tree_statistics = [DynamicHMC.TreeStatisticsNUTS(-65.6441386177003, 1, turning at positions -1:-2, 0.7720493272827138, 3, DynamicHMC.Directions(0x5e73a8a1)), DynamicHMC.TreeStatisticsNUTS(-69.31014546022408, 1, turning at positions -1:-2, 0.5894694006820206, 3, DynamicHMC.Directions(0x901b46f5)), DynamicHMC.TreeStatisticsNUTS(-66.4559977992747, 2, turning at positions -1:2, 0.9194563264060637, 3, DynamicHMC.Directions(0xc71cd5c2)), DynamicHMC.TreeStatisticsNUTS(-64.37290965288247, 1, turning at positions -1:-2, 0.9548406366667795, 3, DynamicHMC.Directions(0xed36e129)), DynamicHMC.TreeStatisticsNUTS(-64.2976947365825, 1, turning at positions 1:2, 0.975426312536316, 3, DynamicHMC.Directions(0x76639c56)), DynamicHMC.TreeStatisticsNUTS(-63.982529341156905, 1, turning at positions -1:0, 1.0, 1, DynamicHMC.Directions(0xf5ef0e92)), DynamicHMC.TreeStatisticsNUTS(-64.08098133716761, 1, turning at positions -1:-2, 0.9829191876937925, 3, DynamicHMC.Directions(0xa6d5a401)), DynamicHMC.TreeStatisticsNUTS(-66.16053124486685, 1, turning at positions -1:-2, 0.719731646132196, 3, DynamicHMC.Directions(0xd23ca0ad)), DynamicHMC.TreeStatisticsNUTS(-65.50735602697235, 2, turning at positions -2:1, 0.9999999999999999, 3, DynamicHMC.Directions(0x42221275)), DynamicHMC.TreeStatisticsNUTS(-64.12100034864153, 1, turning at positions -1:-2, 0.9794299748697844, 3, DynamicHMC.Directions(0x1581f131))  …  DynamicHMC.TreeStatisticsNUTS(-64.13307218094191, 2, turning at positions -2:1, 0.9999999999999999, 3, DynamicHMC.Directions(0x7c734571)), DynamicHMC.TreeStatisticsNUTS(-64.08752851082693, 1, turning at positions 1:2, 0.9778397791587726, 3, DynamicHMC.Directions(0xf728cb02)), DynamicHMC.TreeStatisticsNUTS(-64.96389004959296, 1, turning at positions -2:-3, 0.9074466863347471, 3, DynamicHMC.Directions(0x39826c94)), DynamicHMC.TreeStatisticsNUTS(-65.15929172138053, 2, turning at positions 5:6, 0.9082684840425133, 7, DynamicHMC.Directions(0xbce4b66e)), DynamicHMC.TreeStatisticsNUTS(-65.47385023279003, 1, turning at positions -2:-3, 0.9542479301469196, 3, DynamicHMC.Directions(0x43ffb738)), DynamicHMC.TreeStatisticsNUTS(-63.98991313506381, 2, turning at positions -3:0, 0.992937731089448, 3, DynamicHMC.Directions(0x9a22eee4)), DynamicHMC.TreeStatisticsNUTS(-64.18264303748485, 1, turning at positions 1:2, 0.9666570689915545, 3, DynamicHMC.Directions(0x2abf69ee)), DynamicHMC.TreeStatisticsNUTS(-64.70837260600398, 2, turning at positions 0:3, 0.9379331260275768, 3, DynamicHMC.Directions(0x4fff5f5f)), DynamicHMC.TreeStatisticsNUTS(-64.2861730219423, 2, turning at positions -3:0, 0.9533034131578706, 3, DynamicHMC.Directions(0x8084d26c)), DynamicHMC.TreeStatisticsNUTS(-66.24316725663971, 1, turning at positions -1:-2, 0.7010462480127718, 3, DynamicHMC.Directions(0xa82ce5a1))], κ = Gaussian kinetic energy (Diagonal), √diag(M⁻¹): [0.20160668352485045], ϵ = 0.9293966747054117)\n (posterior_matrix = [0.6490393294987946 0.6490393294987946 … 0.6965187926789422 0.9665634460149771], tree_statistics = [DynamicHMC.TreeStatisticsNUTS(-64.33570665894408, 2, turning at positions -3:0, 0.9674925620310365, 3, DynamicHMC.Directions(0xf0f55ac8)), DynamicHMC.TreeStatisticsNUTS(-64.69496798898601, 1, turning at positions -1:-2, 0.845835506302734, 3, DynamicHMC.Directions(0x5254e299)), DynamicHMC.TreeStatisticsNUTS(-64.05641144327137, 2, turning at positions -1:2, 0.9906070892175628, 3, DynamicHMC.Directions(0x942824fe)), DynamicHMC.TreeStatisticsNUTS(-63.984504566549646, 2, turning at positions -3:0, 0.9929398284723162, 3, DynamicHMC.Directions(0x18f2ac90)), DynamicHMC.TreeStatisticsNUTS(-63.95010396466058, 2, turning at positions -1:2, 0.9999999999999999, 3, DynamicHMC.Directions(0xd246a1f2)), DynamicHMC.TreeStatisticsNUTS(-64.8900635618277, 1, turning at positions -1:-2, 0.8188471715043845, 3, DynamicHMC.Directions(0x7c38df05)), DynamicHMC.TreeStatisticsNUTS(-66.12910573449444, 1, turning at positions 0:1, 0.700645480221775, 1, DynamicHMC.Directions(0x636d1b39)), DynamicHMC.TreeStatisticsNUTS(-65.98241897192041, 1, turning at positions 0:1, 1.0, 1, DynamicHMC.Directions(0x00421d4d)), DynamicHMC.TreeStatisticsNUTS(-65.62328765813363, 1, turning at positions -1:0, 0.9281862805929181, 1, DynamicHMC.Directions(0xba65d7a4)), DynamicHMC.TreeStatisticsNUTS(-65.08638983945981, 1, turning at positions -2:-3, 0.9999999999999999, 3, DynamicHMC.Directions(0x788431a8))  …  DynamicHMC.TreeStatisticsNUTS(-64.41209231434836, 1, turning at positions 0:1, 1.0, 1, DynamicHMC.Directions(0x85fc97d3)), DynamicHMC.TreeStatisticsNUTS(-64.71002582709123, 2, turning at positions 0:3, 0.9225976725393897, 3, DynamicHMC.Directions(0xa43874cf)), DynamicHMC.TreeStatisticsNUTS(-63.98245260863411, 1, turning at positions 0:1, 1.0, 1, DynamicHMC.Directions(0x15e0fd47)), DynamicHMC.TreeStatisticsNUTS(-64.54295418837634, 2, turning at positions 0:3, 0.9019393193688995, 3, DynamicHMC.Directions(0x7c9fb717)), DynamicHMC.TreeStatisticsNUTS(-64.2136551451728, 2, turning at positions -2:1, 0.9836016536197238, 3, DynamicHMC.Directions(0xe529e1f5)), DynamicHMC.TreeStatisticsNUTS(-63.94957773993433, 1, turning at positions 0:1, 1.0, 1, DynamicHMC.Directions(0xf07b8935)), DynamicHMC.TreeStatisticsNUTS(-63.977741335565725, 2, turning at positions 0:3, 0.9937830119424099, 3, DynamicHMC.Directions(0x12475b33)), DynamicHMC.TreeStatisticsNUTS(-63.953738455581714, 2, turning at positions -1:2, 0.9996637424549296, 3, DynamicHMC.Directions(0x1e1eed82)), DynamicHMC.TreeStatisticsNUTS(-63.945386822066716, 2, turning at positions -3:0, 0.9982109257912466, 3, DynamicHMC.Directions(0x788d8624)), DynamicHMC.TreeStatisticsNUTS(-64.79412949400819, 1, turning at positions 1:2, 0.8344157919819111, 3, DynamicHMC.Directions(0x7e78edb6))], κ = Gaussian kinetic energy (Diagonal), √diag(M⁻¹): [0.17815613633808952], ϵ = 1.243456979574239)\n (posterior_matrix = [0.8258541669026621 0.5575518831236066 … 0.7375913559383956 0.7375913559383956], tree_statistics = [DynamicHMC.TreeStatisticsNUTS(-64.45559858086844, 1, turning at positions 0:1, 0.7980720034648046, 1, DynamicHMC.Directions(0x16ca785b)), DynamicHMC.TreeStatisticsNUTS(-64.35348880990301, 2, turning at positions 0:3, 0.9607269397738211, 3, DynamicHMC.Directions(0x0ee73173)), DynamicHMC.TreeStatisticsNUTS(-64.49468011136995, 1, turning at positions 0:1, 0.9487533310430731, 1, DynamicHMC.Directions(0x4acec931)), DynamicHMC.TreeStatisticsNUTS(-64.2777500705689, 1, turning at positions -1:0, 1.0, 1, DynamicHMC.Directions(0x9df64778)), DynamicHMC.TreeStatisticsNUTS(-64.17781633876521, 2, turning at positions -3:0, 0.9687468894175026, 3, DynamicHMC.Directions(0x52f957f0)), DynamicHMC.TreeStatisticsNUTS(-66.65876710792213, 1, turning at positions 1:2, 0.4450488708728844, 3, DynamicHMC.Directions(0x09c6b622)), DynamicHMC.TreeStatisticsNUTS(-63.96697775197918, 1, turning at positions -1:0, 1.0, 1, DynamicHMC.Directions(0x92deacae)), DynamicHMC.TreeStatisticsNUTS(-64.80337303984255, 1, turning at positions -1:-2, 0.6622653329247822, 3, DynamicHMC.Directions(0x93540e95)), DynamicHMC.TreeStatisticsNUTS(-63.95257283896458, 2, turning at positions -1:2, 0.9999999999999999, 3, DynamicHMC.Directions(0xe31d603e)), DynamicHMC.TreeStatisticsNUTS(-63.93879995433242, 1, turning at positions -1:0, 1.0, 1, DynamicHMC.Directions(0xcfdf4f2e))  …  DynamicHMC.TreeStatisticsNUTS(-64.27659563424389, 2, turning at positions 0:3, 0.9188134128867608, 3, DynamicHMC.Directions(0xc44edef3)), DynamicHMC.TreeStatisticsNUTS(-64.29960013429316, 2, turning at positions -2:1, 0.9864236692102905, 3, DynamicHMC.Directions(0x81e8a051)), DynamicHMC.TreeStatisticsNUTS(-64.4595092217462, 2, turning at positions -3:0, 0.896766132738969, 3, DynamicHMC.Directions(0x022ec2c8)), DynamicHMC.TreeStatisticsNUTS(-64.92777293888273, 2, turning at positions 0:3, 0.8959483941496461, 3, DynamicHMC.Directions(0x913740df)), DynamicHMC.TreeStatisticsNUTS(-68.13119973569665, 1, turning at positions 1:2, 0.284844612562679, 3, DynamicHMC.Directions(0x0bb22692)), DynamicHMC.TreeStatisticsNUTS(-64.29440258085137, 1, turning at positions -1:0, 1.0, 1, DynamicHMC.Directions(0xae465eae)), DynamicHMC.TreeStatisticsNUTS(-64.91622011755443, 2, turning at positions 0:3, 0.8689941143721458, 3, DynamicHMC.Directions(0xbdac840f)), DynamicHMC.TreeStatisticsNUTS(-64.30137424533171, 2, turning at positions -3:0, 0.9736722554455438, 3, DynamicHMC.Directions(0x7774d164)), DynamicHMC.TreeStatisticsNUTS(-64.09713434217794, 1, turning at positions 1:2, 0.9320772736998718, 3, DynamicHMC.Directions(0xcb6a5ad6)), DynamicHMC.TreeStatisticsNUTS(-64.17571122470792, 1, turning at positions -1:-2, 0.9025924298481754, 3, DynamicHMC.Directions(0x5c9cbd7d))], κ = Gaussian kinetic energy (Diagonal), √diag(M⁻¹): [0.2154995848145599], ϵ = 1.211583933253109)\n (posterior_matrix = [0.6873448457499447 0.8561791981399807 … 0.7799115365263016 0.7181014661468901], tree_statistics = [DynamicHMC.TreeStatisticsNUTS(-64.0899543759111, 1, turning at positions 2:3, 0.9999999999999999, 3, DynamicHMC.Directions(0x41fe8c43)), DynamicHMC.TreeStatisticsNUTS(-64.24910349510643, 1, turning at positions 1:2, 0.9438582917952042, 3, DynamicHMC.Directions(0xb8cde62a)), DynamicHMC.TreeStatisticsNUTS(-64.27053744809938, 1, turning at positions 0:1, 0.9746870970955887, 1, DynamicHMC.Directions(0x5e13a73b)), DynamicHMC.TreeStatisticsNUTS(-64.31638990613621, 1, turning at positions 2:3, 0.9865436439667566, 3, DynamicHMC.Directions(0xbffc2b3b)), DynamicHMC.TreeStatisticsNUTS(-64.09453350582963, 2, turning at positions -3:0, 0.9763883060544166, 3, DynamicHMC.Directions(0x98019cd4)), DynamicHMC.TreeStatisticsNUTS(-65.84569881405254, 2, turning at positions -3:0, 0.7890431583077278, 3, DynamicHMC.Directions(0x62f33c6c)), DynamicHMC.TreeStatisticsNUTS(-66.76030908478285, 1, turning at positions 0:1, 0.6293904371754105, 1, DynamicHMC.Directions(0xf3b7fd01)), DynamicHMC.TreeStatisticsNUTS(-67.12137540856217, 2, turning at positions -1:2, 0.8834343340417442, 3, DynamicHMC.Directions(0xaa0834a6)), DynamicHMC.TreeStatisticsNUTS(-67.08389911888314, 1, turning at positions -1:0, 1.0, 1, DynamicHMC.Directions(0x54e41500)), DynamicHMC.TreeStatisticsNUTS(-66.38239561255686, 1, turning at positions 0:1, 1.0, 1, DynamicHMC.Directions(0xfc3bd62d))  …  DynamicHMC.TreeStatisticsNUTS(-63.948536831342665, 1, turning at positions -1:0, 1.0, 1, DynamicHMC.Directions(0xd2ce8bb0)), DynamicHMC.TreeStatisticsNUTS(-63.94356523899352, 2, turning at positions -1:2, 0.9999999999999999, 3, DynamicHMC.Directions(0xae63c47a)), DynamicHMC.TreeStatisticsNUTS(-64.03134340843145, 1, turning at positions 1:2, 0.9821713505337627, 3, DynamicHMC.Directions(0xbb01987e)), DynamicHMC.TreeStatisticsNUTS(-65.7737688815397, 1, turning at positions 1:2, 0.7035817371363562, 3, DynamicHMC.Directions(0xd8de9afa)), DynamicHMC.TreeStatisticsNUTS(-65.56700062642435, 1, turning at positions -2:-3, 0.9951650149470072, 3, DynamicHMC.Directions(0x5e734b28)), DynamicHMC.TreeStatisticsNUTS(-63.950477374023464, 2, turning at positions -3:0, 0.9976201265242027, 3, DynamicHMC.Directions(0xfe7fee5c)), DynamicHMC.TreeStatisticsNUTS(-64.65491855836747, 2, turning at positions 0:3, 0.9045389943071404, 3, DynamicHMC.Directions(0xbd086b5b)), DynamicHMC.TreeStatisticsNUTS(-64.33082949204807, 1, turning at positions 2:3, 0.9999999999999999, 3, DynamicHMC.Directions(0x34aa2ecf)), DynamicHMC.TreeStatisticsNUTS(-64.12056701580758, 1, turning at positions -2:-3, 0.987780262916181, 3, DynamicHMC.Directions(0x69b61ee0)), DynamicHMC.TreeStatisticsNUTS(-65.11412378208883, 2, turning at positions 0:3, 0.8357983830962304, 3, DynamicHMC.Directions(0x8437b34b))], κ = Gaussian kinetic energy (Diagonal), √diag(M⁻¹): [0.19834202069771587], ϵ = 1.0727696857905928)\n```\n:::\n:::\n\n\nThe `result` is a vector of length $k$, each element of which contains for each chain  the posterior samples as well as some statistics about the sampling procedure, which can be used to check if everything went as planned.\n\n## Model checking\n\nHaving obtained samples from the posterior distribution, we're in principle ready to use our model for inference, i.e., answer the question of whether our coin is biased and by how much, and how certain we can be of the answer based on the data we have seen.\n\nHowever, before we jump to inference, it is good practice to perform some model checks: Our estimates rely on a numerical sampling scheme, which can fail, rendering the results unreliable. \n\n::: {.cell execution_count=13}\n``` {.julia .cell-code}\nusing MCMCDiagnosticTools\nusing DynamicHMC.Diagnostics\n```\n:::\n\n\nFirst, we can check the effective sample size (ess). In Markov chain monte carlo (MCMC) approaches, samples are often correlated, meaning that the total number of 'effective' samples is less than obtained by an uncorrelated sampling procedure because consecutive samples carry some of the same information.\n\n::: {.cell execution_count=14}\n``` {.julia .cell-code}\ness, Rhat =  ess_rhat(stack_posterior_matrices(result))\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n([2964.8114829505657], [1.0014512557391575])\n```\n:::\n:::\n\n\n::: {.cell execution_count=15}\n``` {.julia .cell-code}\nsummarize_tree_statistics.(getfield.(result, :tree_statistics))\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\n4-element Vector{DynamicHMC.Diagnostics.TreeStatisticsSummary{Float64, NamedTuple{(:max_depth, :divergence, :turning), Tuple{Int64, Int64, Int64}}}}:\n Hamiltonian Monte Carlo sample of length 2000\n  acceptance rate mean: 0.94, 5/25/50/75/95%: 0.75 0.91 0.98 1.0 1.0\n  termination: divergence => 0%, max_depth => 0%, turning => 100%\n  depth: 0 => 0%, 1 => 61%, 2 => 39%\n Hamiltonian Monte Carlo sample of length 2000\n  acceptance rate mean: 0.91, 5/25/50/75/95%: 0.63 0.88 0.97 1.0 1.0\n  termination: divergence => 0%, max_depth => 0%, turning => 100%\n  depth: 0 => 0%, 1 => 61%, 2 => 39%\n Hamiltonian Monte Carlo sample of length 2000\n  acceptance rate mean: 0.87, 5/25/50/75/95%: 0.48 0.81 0.96 1.0 1.0\n  termination: divergence => 0%, max_depth => 0%, turning => 100%\n  depth: 0 => 0%, 1 => 58%, 2 => 42%\n Hamiltonian Monte Carlo sample of length 2000\n  acceptance rate mean: 0.92, 5/25/50/75/95%: 0.67 0.89 0.98 1.0 1.0\n  termination: divergence => 0%, max_depth => 0%, turning => 100%\n  depth: 0 => 0%, 1 => 64%, 2 => 36%\n```\n:::\n:::\n\n\n## Model inference\n\n::: {.cell execution_count=16}\n``` {.julia .cell-code}\nusing StructArrays\n\nfunction posterior(result)\n  samples = eachcol(pool_posterior_matrices(result))\n  StructArray(transform.(transformation, samples))\nend\n\npost = posterior(result);\n```\n:::\n\n\n::: {.cell execution_count=17}\n``` {.julia .cell-code}\nfunction summarize(post)\n  m, s = round.((mean(post.p), std(post.p)); digits=2)\n  println(\"posterior mean: \", m)\n  println(\"posterior sd: \", s)\nend\n\nsummarize(post)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nposterior mean: 0.67\nposterior sd: 0.05\n```\n:::\n:::\n\n\n::: {.cell execution_count=18}\n``` {.julia .cell-code}\nfunction plot_inferred_vs_true(post, p_true)\n  fig = Figure(); ax = Axis(fig[1,1])\n  density!(ax, post.p; color=:grey20)\n  vlines!(ax, p_true; linewidth=2)\n  fig\nend\n\nplot_inferred_vs_true(post, p)\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n![](coinflip_ldp_files/figure-html/cell-19-output-1.svg){}\n:::\n:::\n\n\n",
    "supporting": [
      "coinflip_ldp_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}